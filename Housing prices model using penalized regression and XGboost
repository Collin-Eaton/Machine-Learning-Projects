import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score

#Uploading data
housedata=pd.read_csv('train.csv')
housedata1=pd.read_csv('test.csv')

#exploring data
housedata.head()
housedata.info()

print("Shape: {}".format(str(housedata.shape)))
housedata.describe()

#checking correlation for variables
corr_matrix = housedata.corr()
corr_matrix['SalePrice'].sort_values(ascending = False)

#dropping uncorrelated variables from dataset
housedata = housedata.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'PoolArea'], axis = 1)
housedata = housedata.drop(['ScreenPorch', 'MoSold','3SsnPorch','BsmtFinSF2','BsmtHalfBath','MiscVal','Id','LowQualFinSF','YrSold','OverallCond','MSSubClass','EnclosedPorch','KitchenAbvGr'], axis = 1)

#creating two lists one for categorical variables and one for quantitative 
quant_list = list(housedata.select_dtypes(include = [np.int64, np.float64]).columns)
category_list=list(housedata.select_dtypes(include = [object]).columns)
df_quant = housedata.drop(category_list, axis=1)
category_df= housedata.drop(quant_list, axis = 1)
category_dummies = pd.get_dummies(category_df)
final_dataframe = pd.concat([df_quant1, category_dummies], axis = 1, join = 'outer')
final_dataframe.dtypes

#creating final dependent and independent variables
X=final_dataframe.drop('SalePrice', axis=1)
Y=final_dataframe['SalePrice']

#getting best parameters for penalized regression
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
ridge = Ridge().fit(X, Y)
alpha_grid = {'alpha': [.002, .004, .006, .008, .01, .012, .014, .016 ,.018, .02 ],'max_iter': [100000]}
grid_search = GridSearchCV(ridge,alpha_grid,cv=5,return_train_score=True)

#fitting model with best parameter and getting MSE for test model
best_model=grid_search.fit(X,Y)
print("Best alpha Ridge Train: ",best_model.best_estimator_.get_params()['alpha'])
print("MSE Train:",best_model.best_score_)

#Getting best parameters with XGboost and Testing MSE 
xgb = XGBRegressor(n_estimators=1000, max_depth=5, learning_rate=0.1)
scores = cross_val_score(xgb, X, Y, cv=10)

print(scores)
print(scores.mean())

model.fit(X, Y)
xgb_preds = model.predict(X)
error = mean_squared_error(Y, xgb_preds)
print(error)






