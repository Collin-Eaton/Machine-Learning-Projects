{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "# importing pyspark and starting a session to use pyspark\nimport pyspark\nfrom pyspark.sql import SparkSession\nsc = pyspark.SparkContext.getOrCreate()"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "mkdir: `/input1': File exists\n"}], "source": "# Making a directory to save my stack overflow query to use as input\n!hdfs dfs -mkdir /input1"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "copyFromLocal: `/input1/Query.csv': File exists\n"}], "source": "# Saving Stack overflow query file to new directory\n!hdfs dfs -copyFromLocal Query.csv /input1"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 2 items\n-rw-r--r--   2 root hadoop     147147 2021-03-04 04:55 /input1/Query.csv\n-rw-r--r--   2 root hadoop     348070 2021-02-27 22:04 /input1/QueryResults.csv\n"}], "source": "# Checking to make sure the file was saved correctly to the directory\n!hdfs dfs -ls /input1"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "# Taking the query and making it workable by creating an RDD\nmyrdd = sc.textFile(\"/input1/Query.csv\") "}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/plain": "True"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "#Checking to make sure my file was created as an RDD\nfrom pyspark.rdd import RDD\nisinstance(myrdd, RDD)"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"data": {"text/plain": "['<google-cloud-messaging>\"',\n '\"50682076\",\"<java><spring><oauth-2.0><spring-security-oauth2>\"',\n '\"50682080\",\"<dialogflow-es>\"',\n '\"50682087\",\"<spring-mvc><jstl>\"',\n '\"50682089\",\"<reactjs><amazon-web-services><amazon-s3><amazon-sns>\"']"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "#Checking first five lines of RDD\nmyrdd.take(5)"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "# importing function to get rid of header in file\nfrom itertools import islice\n\n# Getting rid of the header row by slicing the first line of the RDD\nmyrdd = myrdd.mapPartitionsWithIndex(\n    lambda idx, it: islice(it, 1, None) if idx == 0 else it)"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"data": {"text/plain": "['\"50682076\",\"<java><spring><oauth-2.0><spring-security-oauth2>\"',\n '\"50682080\",\"<dialogflow-es>\"',\n '\"50682087\",\"<spring-mvc><jstl>\"',\n '\"50682089\",\"<reactjs><amazon-web-services><amazon-s3><amazon-sns>\"',\n '\"50682093\",\"<batch-file>\"']"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "# Checking to make sure the header line was correctly taken off\nmyrdd.take(5)"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"data": {"text/plain": "[['50682076', 'java', 'spring', 'oauth-2.0', 'spring-security-oauth2'],\n ['50682080', 'dialogflow-es'],\n ['50682087', 'spring-mvc', 'jstl'],\n ['50682089', 'reactjs', 'amazon-web-services', 'amazon-s3', 'amazon-sns'],\n ['50682093', 'batch-file']]"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "# cleaning up the data by creating an index of characters to get rid of \nchar_to_replace = {'<': ' ',\n                   '>': ' ',\n                   '\"': '',\n                  ',':''}\n\n# Making my rdd data all lowercase, and using the translate function to translate all characters created in the previous index to blanks\nwordlist = myrdd.map(lambda x: x.lower().translate(str.maketrans(char_to_replace)).split())\nwordlist.take(5)"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"data": {"text/plain": "['50682076', '50682080', '50682087', '50682089', '50682093']"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "# Testing the lambda function to see if I can pull the first value of each line in the RDD\nword_tuple = wordlist.map(lambda x: x[0])\nword_tuple.take(5)"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": "# Testing the lamda function with a for loop to take the first value and making a pair with all other values in each line\nword_tuple1 = wordlist.map(lambda list: [(word, list[0]) for word in list])"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"data": {"text/plain": "[[('50682076', '50682076'),\n  ('java', '50682076'),\n  ('spring', '50682076'),\n  ('oauth-2.0', '50682076'),\n  ('spring-security-oauth2', '50682076')],\n [('50682080', '50682080'), ('dialogflow-es', '50682080')],\n [('50682087', '50682087'), ('spring-mvc', '50682087'), ('jstl', '50682087')],\n [('50682089', '50682089'),\n  ('reactjs', '50682089'),\n  ('amazon-web-services', '50682089'),\n  ('amazon-s3', '50682089'),\n  ('amazon-sns', '50682089')],\n [('50682093', '50682093'), ('batch-file', '50682093')]]"}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": "#checking results\nword_tuple1.take(5)"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "# Using a lambda function with a for loop to create a tuple for each element of each line along with the first element starting with the second value so I don't make the first value a tuple with itself\n# Basically takeing the length of list minus one to not loop through the first value but to get the index of each element with the first element\nword_tuple2 = wordlist.map(lambda list: [((i +1 % len(list)), list[0]) for i in (range(len(list)-1))])"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"data": {"text/plain": "[[(1, '50682076'), (2, '50682076'), (3, '50682076'), (4, '50682076')],\n [(1, '50682080')],\n [(1, '50682087'), (2, '50682087')],\n [(1, '50682089'), (2, '50682089'), (3, '50682089'), (4, '50682089')],\n [(1, '50682093')]]"}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": "#checking results\nword_tuple2.take(5)"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": "#adding list to the front of the above lambda function to reference the value of the elements rather than the order of where they fall in the list\nword_tuple3 = wordlist.map(lambda list: [(list[(i +1 % len(list))], list[0]) for i in (range(len(list)-1))])"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"data": {"text/plain": "[[('java', '50682076'),\n  ('spring', '50682076'),\n  ('oauth-2.0', '50682076'),\n  ('spring-security-oauth2', '50682076')],\n [('dialogflow-es', '50682080')],\n [('spring-mvc', '50682087'), ('jstl', '50682087')],\n [('reactjs', '50682089'),\n  ('amazon-web-services', '50682089'),\n  ('amazon-s3', '50682089'),\n  ('amazon-sns', '50682089')],\n [('batch-file', '50682093')]]"}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": "#checking results\nword_tuple3.take(5)"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": "# Now running the same lamda function inside of a flatmap function to get rid of the list of lists and make a list of all tuples\nword_tuple4 = word_tuple3.flatMap(lambda tuplelist: [(tuple) for tuple in tuplelist])"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"data": {"text/plain": "[('java', '50682076'),\n ('spring', '50682076'),\n ('oauth-2.0', '50682076'),\n ('spring-security-oauth2', '50682076'),\n ('dialogflow-es', '50682080'),\n ('spring-mvc', '50682087'),\n ('jstl', '50682087'),\n ('reactjs', '50682089'),\n ('amazon-web-services', '50682089'),\n ('amazon-s3', '50682089'),\n ('amazon-sns', '50682089'),\n ('batch-file', '50682093'),\n ('sql', '50683180'),\n ('sql-server', '50683180'),\n ('date', '50683180')]"}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": "word_tuple4.take(15)"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": "# Reducer function to take each tag and make a list of all post id's that include the tag key.\nword_tuple5 = word_tuple4.groupByKey()"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"data": {"text/plain": "[('java', <pyspark.resultiterable.ResultIterable at 0x7f0b34348a90>),\n ('oauth-2.0', <pyspark.resultiterable.ResultIterable at 0x7f0b34348990>),\n ('spring-security-oauth2',\n  <pyspark.resultiterable.ResultIterable at 0x7f0b34059910>),\n ('jstl', <pyspark.resultiterable.ResultIterable at 0x7f0b34059750>),\n ('reactjs', <pyspark.resultiterable.ResultIterable at 0x7f0b34059a90>),\n ('amazon-web-services',\n  <pyspark.resultiterable.ResultIterable at 0x7f0b34056dd0>),\n ('amazon-sns', <pyspark.resultiterable.ResultIterable at 0x7f0b34058750>),\n ('sql-server', <pyspark.resultiterable.ResultIterable at 0x7f0b340597d0>),\n ('es6-promise', <pyspark.resultiterable.ResultIterable at 0x7f0b3405a910>),\n ('scss-mixins', <pyspark.resultiterable.ResultIterable at 0x7f0b3405a9d0>),\n ('resttemplate', <pyspark.resultiterable.ResultIterable at 0x7f0b3405aa90>),\n ('core-data', <pyspark.resultiterable.ResultIterable at 0x7f0b3405ab50>),\n ('visual-studio', <pyspark.resultiterable.ResultIterable at 0x7f0b3405ad50>),\n ('django', <pyspark.resultiterable.ResultIterable at 0x7f0b3405b250>),\n ('prismjs', <pyspark.resultiterable.ResultIterable at 0x7f0b3405bd90>)]"}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}], "source": "# Checking results to see if the inverted index has been created\nword_tuple5.take(15)"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"data": {"text/plain": "2928"}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": "# Checking the count of all the different tags\nword_tuple5.count()"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "mkdir: `/output1': File exists\n"}], "source": "# Creating an output file to save the inverted index to\n!hdfs dfs -mkdir /output1"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": "# Saving my RDD back to disk in my output directory\nword_tuple5.saveAsTextFile('/output1/spark_results2')"}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 3 items\n-rw-r--r--   2 root hadoop          0 2021-03-05 19:14 /output1/spark_results1/_SUCCESS\n-rw-r--r--   2 root hadoop     119480 2021-03-05 19:14 /output1/spark_results1/part-00000\n-rw-r--r--   2 root hadoop     118544 2021-03-05 19:14 /output1/spark_results1/part-00001\n"}], "source": "# Checking that the file saved correctly to my directory \n!hdfs dfs -ls /output1/spark_results1"}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "('java', <pyspark.resultiterable.ResultIterable object at 0x7f3dc38d83d0>)\n('oauth-2.0', <pyspark.resultiterable.ResultIterable object at 0x7f3dc38d83d0>)\n('spring-security-oauth2', <pyspark.resultiterable.ResultIterable object at 0x7f3dc38d83d0>)\n('jstl', <pyspark.resultiterable.ResultIterable object at 0x7f3dc38d83d0>)\n('reactjs', <pyspark.resultiterable.ResultIterable object at 0x7f3dc38d83d0>)\n('amazon-web-services', <pyspark.resultiterable.ResultIterable object at 0x7f3dc38d83d0>)\n('amazon-sns', <pyspark.resultiterable.ResultIterable object at 0x7f3dc38d83d0>)\n('sql-server', <pyspark.resultiterable.ResultIterable object at 0x7f3dc38d83d0>)\n('es6-promise', <pyspark.resultiterable.ResultIterable object at 0x7f3dc38d83d0>)\n('scss-mixins', <pyspark.resultiterable.ResultIterable object at 0x7f3dc38d83d0>)\ncat: Unable to write to output stream.\n"}], "source": "# checking one of the nodes to see what was saved\n!hdfs dfs -cat /output1/spark_results/part-00000 | head"}, {"cell_type": "code", "execution_count": 89, "metadata": {}, "outputs": [], "source": "# putting data into datframe\nimport pandas as pd\ndf = pd.DataFrame(data, columns =['Tag', 'Id'])"}, {"cell_type": "code", "execution_count": 90, "metadata": {}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tag</th>\n      <th>Id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>java</td>\n      <td>(50682076, 50683730, 50683786, 50684395, 50684...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>oauth-2.0</td>\n      <td>(50682076, 50686068, 50635455)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spring-security-oauth2</td>\n      <td>(50682076)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>jstl</td>\n      <td>(50682087, 50734468)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>reactjs</td>\n      <td>(50682089, 50683201, 50684367, 50684628, 50684...</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "                      Tag                                                 Id\n0                    java  (50682076, 50683730, 50683786, 50684395, 50684...\n1               oauth-2.0                     (50682076, 50686068, 50635455)\n2  spring-security-oauth2                                         (50682076)\n3                    jstl                               (50682087, 50734468)\n4                 reactjs  (50682089, 50683201, 50684367, 50684628, 50684..."}, "execution_count": 90, "metadata": {}, "output_type": "execute_result"}], "source": "#checking dataframe\ndf.head()"}, {"cell_type": "code", "execution_count": 109, "metadata": {}, "outputs": [], "source": "df['Id'] = df['Id'].astype(str)"}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [], "source": "df.to_csv('index.csv') "}, {"cell_type": "code", "execution_count": 116, "metadata": {}, "outputs": [], "source": "myrdd1 = sc.textFile('index.csv')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}